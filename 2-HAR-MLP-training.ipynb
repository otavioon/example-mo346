{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook explores how to train an MLP model to solve the HAR task\n",
    "\n",
    "##  1. Instantiating the datamodule object and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/har/train.csv file is missing\n",
      "Creating the root data directory: [data/har]\n",
      "Could not find the zip file [data/har/har.zip]\n",
      "Trying to download it.\n",
      "Data downloaded and extracted\n"
     ]
    }
   ],
   "source": [
    "from data_modules.har import HarDataModule\n",
    "\n",
    "# Instantiating the HarDataModule with root dir at data/example\n",
    "my_datamodule = HarDataModule(root_data_dir=\"data/har\", \n",
    "                              flatten = True, \n",
    "                              target_column = \"standard activity code\", \n",
    "                              batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the object is instantiated, we shall invoke the prepare_data() method to ensure the data is downloaded and extracted at the root_data_dir. Once you execute this command, you may inspect the data/example directory for CSV data files.\n",
    "my_datamodule.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieving the training and validation set dataloaders\n",
    "\n",
    "Besides defining the batch size, the data module manages the data loaders for the training, validation, and test partitions. \n",
    "The next command shows how to retrieve the training and validation data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the training set dataloader (dl)\n",
    "train_dl = my_datamodule.train_dataloader()\n",
    "val_dl = my_datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating the ML model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (block): Sequential(\n",
      "    (0): Linear(in_features=360, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=6, bias=True)\n",
      "  )\n",
      "  (loss): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models.mlp import MultiLayerPerceptron\n",
    "\n",
    "model = MultiLayerPerceptron(input_features = 360, \n",
    "                             hidden_size = 64,\n",
    "                             num_classes = 6)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the untrained model\n",
    "\n",
    "First, lets test the model in a very detailed way, so we can inspect the shape of the tensors being processed and produced.\n",
    "NOTE: the model has not been trained yet, hence, we expect very poor results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Processing the batch 0\n",
      " X.shape =  torch.Size([16, 360])\n",
      " y.shape =  torch.Size([16])\n",
      " logits.shape =  torch.Size([16, 6])\n",
      " predictions.shape =  torch.Size([16])\n",
      " predictions =  tensor([0, 3, 0, 4, 0, 0, 2, 2, 2, 0, 2, 0, 2, 2, 0, 0])\n",
      " labels      =  tensor([2, 5, 5, 4, 2, 2, 0, 2, 0, 2, 1, 2, 0, 3, 4, 4])\n",
      " correct     =  tensor([False, False, False,  True, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False])\n",
      "-- Processing the batch 1\n",
      " X.shape =  torch.Size([16, 360])\n",
      " y.shape =  torch.Size([16])\n",
      " logits.shape =  torch.Size([16, 6])\n",
      " predictions.shape =  torch.Size([16])\n",
      " predictions =  tensor([2, 3, 2, 2, 0, 0, 0, 3, 4, 2, 3, 4, 3, 2, 4, 3])\n",
      " labels      =  tensor([0, 5, 2, 1, 3, 3, 3, 3, 5, 1, 5, 3, 2, 0, 5, 5])\n",
      " correct     =  tensor([False, False,  True, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False])\n",
      "-- Processing the batch 2\n",
      " X.shape =  torch.Size([16, 360])\n",
      " y.shape =  torch.Size([16])\n",
      " logits.shape =  torch.Size([16, 6])\n",
      " predictions.shape =  torch.Size([16])\n",
      " predictions =  tensor([2, 2, 2, 0, 2, 3, 1, 2, 2, 2, 0, 3, 0, 2, 2, 2])\n",
      " labels      =  tensor([0, 1, 1, 1, 0, 3, 4, 4, 1, 0, 3, 5, 4, 0, 5, 0])\n",
      " correct     =  tensor([False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False])\n",
      "-- Processing the batch 3\n",
      " X.shape =  torch.Size([12, 360])\n",
      " y.shape =  torch.Size([12])\n",
      " logits.shape =  torch.Size([12, 6])\n",
      " predictions.shape =  torch.Size([12])\n",
      " predictions =  tensor([0, 4, 3, 3, 0, 2, 0, 0, 0, 0, 2, 0])\n",
      " labels      =  tensor([1, 5, 4, 3, 2, 1, 2, 3, 4, 4, 1, 4])\n",
      " correct     =  tensor([False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False])\n",
      "Total number of predictions   = 60\n",
      "Number of correct predictions = 6\n",
      "Accuracy =  tensor(0.1000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize counters:\n",
    "number_of_predictions = 0\n",
    "number_of_correct_predictions = 0\n",
    "\n",
    "# For each batch, compute the predictions and compare with the labels.\n",
    "for batch_idx, (X, y) in enumerate(train_dl):\n",
    "    print(f\"-- Processing the batch {batch_idx}\")\n",
    "\n",
    "    # X and y contains the input features and the expected labels for the B samples in the batch\n",
    "    # Lets print their shape\n",
    "    print(\" X.shape = \", X.shape)\n",
    "    print(\" y.shape = \", y.shape)\n",
    "\n",
    "    # Invoke the mlp model produces the logits. Logits are the raw scores output by the last layer \n",
    "    # of the neural network before applying the softmax function to convert them into probabilities. \n",
    "    # In this case, it is an array with 6 raw scores.\n",
    "    # Note: the following code invokes the forward() method from the MultiLayerPerceptron class\n",
    "    logits = model(X)\n",
    "    print(\" logits.shape = \", logits.shape)\n",
    "\n",
    "    # We'll utilize the argmax function to determine the index of the array position with the highest score. \n",
    "    # For instance, in the array argmax([2, 8, 15, 0, 3, 9]) == 2, as the value 15 is located at index 2\n",
    "    predictions = torch.argmax(logits, dim=1)  \n",
    "\n",
    "    # Now, we have predictions for each one of the B input samples. \n",
    "    # Lets print the shape of the predictions tensor and print the predictions themselves.    \n",
    "    print(\" predictions.shape = \", predictions.shape)\n",
    "    print(\" predictions = \", predictions)\n",
    "\n",
    "    # Next, lets compare the predictions against the expected labels\n",
    "    correct = (predictions == y)\n",
    "    print(\" labels      = \", y)\n",
    "    print(\" correct     = \", correct)\n",
    "\n",
    "    # Finally, lets count the total number of correct predictions and the total number of predictions.\n",
    "    number_of_predictions += int(X.shape[0]) # Number of elements in the batch\n",
    "    number_of_correct_predictions += correct.sum() # Number of True values in correct\n",
    "\n",
    "print(f\"Total number of predictions   = {number_of_predictions}\")\n",
    "print(f\"Number of correct predictions = {number_of_correct_predictions}\")\n",
    "print(\"Accuracy = \", number_of_correct_predictions/number_of_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take some time to inspect the output of the previous code block and make sure it makes sense.\n",
    "Next, lets perform the same test, but without printing all the intermediate values.\n",
    "We will also encapsulate the code in a function so we can reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset_dl):\n",
    "    # Initialize counters:\n",
    "    number_of_predictions = 0\n",
    "    number_of_correct_predictions = 0\n",
    "\n",
    "    # For each batch, compute the predictions and compare with the labels.\n",
    "    for X, y in dataset_dl:\n",
    "        logits = model(X)\n",
    "        predictions = torch.argmax(logits, dim=1)  \n",
    "        correct = (predictions == y)\n",
    "        number_of_predictions += int(X.shape[0])\n",
    "        number_of_correct_predictions += correct.sum()\n",
    "    # Return a tuple with the number of correct predictions and the total number of predictions\n",
    "    return (int(number_of_correct_predictions), int(number_of_predictions))\n",
    "\n",
    "def report_accuracy(model, dataset_dl, prefix=\"\"):\n",
    "    number_of_correct_predictions, number_of_predictions = evaluate_model(model, dataset_dl)\n",
    "    print(prefix+\"Accuracy = {:0.2f} % ({}/{})\".format(100*number_of_correct_predictions/number_of_predictions,\n",
    "                                             number_of_correct_predictions, \n",
    "                                             number_of_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset Accuracy = 10.00 % (6/60)\n",
      "Validation dataset Accuracy = 4.17 % (1/24)\n"
     ]
    }
   ],
   "source": [
    "report_accuracy(model, train_dl, prefix=\"Training dataset \")\n",
    "report_accuracy(model, val_dl, prefix=\"Validation dataset \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the model\n",
    "\n",
    "In this section, we will explore how a model is trained with pytorch.\n",
    "\n",
    "First, lets define the optimizer and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will employ pytorch SGD optimizer. We must provide it with the model parameters and the learning rate.\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets instanciate the loss function. In this case, we will employ the pytorch CrossEntropyLoss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model involves:\n",
    "\n",
    "- Making predictions on the training samples.\n",
    "- Calculating the gradients with respect to the loss functions.\n",
    "- Adjusting the model parameters by applying the gradients scaled by the learning rate.\n",
    "\n",
    "Lets first try it on a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =  torch.Size([16, 360])\n",
      "y.shape =  torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Fetch the first batch\n",
    "first_batch = next(iter(train_dl))\n",
    "# Fetch the first element on the batch (X_0, y_0)\n",
    "X, y = first_batch\n",
    "# Print the shape and contents of the batch\n",
    "print(\"X.shape = \", X.shape)\n",
    "print(\"y.shape = \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 31.67 % (19/60)\n"
     ]
    }
   ],
   "source": [
    "# Set the model in train mode -- this makes the forward pass store intermediate results that are required by the backward pass.\n",
    "model.train()\n",
    "\n",
    "# Perform the predictions (forward pass)\n",
    "logits = model(X)\n",
    "\n",
    "# Compute the loss\n",
    "loss = loss_fn(logits, y)\n",
    "\n",
    "# Reset the gradient values. The gradients are stored next to the model parameters, \n",
    "# on the same tensor objects, but on different attributes.\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Perform the backwards pass -- it computes and accumulates the gradients\n",
    "loss.backward()\n",
    "\n",
    "# Adjust the model weights according to the computed gradients and the learning rate.\n",
    "optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "report_accuracy(model, train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code compute the gradients and updated the model weights using only a subset (a batch) of the training set. \n",
    "\n",
    "The next code shows how to train the model using all the batches and for multiple epochs. \n",
    "Notice that each epoch consists of training the model with all batches in the dataset (hence, the second loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ** Epoch 0 **\n",
      "   Training Accuracy = 75.00 % (45/60)\n",
      "   Validation Accuracy = 33.33 % (8/24)\n",
      " ** Epoch 1 **\n",
      "   Training Accuracy = 78.33 % (47/60)\n",
      "   Validation Accuracy = 37.50 % (9/24)\n",
      " ** Epoch 2 **\n",
      "   Training Accuracy = 83.33 % (50/60)\n",
      "   Validation Accuracy = 33.33 % (8/24)\n",
      " ** Epoch 3 **\n",
      "   Training Accuracy = 83.33 % (50/60)\n",
      "   Validation Accuracy = 33.33 % (8/24)\n",
      " ** Epoch 4 **\n",
      "   Training Accuracy = 83.33 % (50/60)\n",
      "   Validation Accuracy = 37.50 % (9/24)\n",
      " ** Epoch 5 **\n",
      "   Training Accuracy = 83.33 % (50/60)\n",
      "   Validation Accuracy = 37.50 % (9/24)\n",
      " ** Epoch 6 **\n",
      "   Training Accuracy = 83.33 % (50/60)\n",
      "   Validation Accuracy = 37.50 % (9/24)\n",
      " ** Epoch 7 **\n",
      "   Training Accuracy = 83.33 % (50/60)\n",
      "   Validation Accuracy = 37.50 % (9/24)\n",
      " ** Epoch 8 **\n",
      "   Training Accuracy = 83.33 % (50/60)\n",
      "   Validation Accuracy = 33.33 % (8/24)\n",
      " ** Epoch 9 **\n",
      "   Training Accuracy = 83.33 % (50/60)\n",
      "   Validation Accuracy = 33.33 % (8/24)\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 10\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    print(f\" ** Epoch {epoch} **\")\n",
    "\n",
    "    # Set the model in train mode. \n",
    "    model.train()\n",
    "\n",
    "    for X,y in train_dl:\n",
    "        # Perform the predictions (forward pass)\n",
    "        logits = model(X)\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(logits, y)\n",
    "        # Reset the gradient values.\n",
    "        optimizer.zero_grad()\n",
    "        # Perform the backwards pass\n",
    "        loss.backward()\n",
    "        # Adjust the model weights according to the computed gradients and the learning rate.\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Set the model in evaluation mode for faster evaluation\n",
    "    model.eval()\n",
    "    report_accuracy(model, train_dl, prefix=\"   Training \")\n",
    "    report_accuracy(model, val_dl, prefix=\"   Validation \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has been trained for 10 epochs. Notice that the training accuracy increased while the validation accuracy might have increased up to a point and then decreased again (hint: this might characterize overfitting). \n",
    "\n",
    "You might want to change the previous code to record the loss and accuracy values so you can plot them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the model using [Pytorch Lightning](https://lightning.ai/docs/pytorch/stable/)\n",
    "\n",
    "[Pytorch Lightning](https://lightning.ai/docs/pytorch/stable/) provides an API to facilitate training ML models with Pytorch.\n",
    "\n",
    "Before continuing, please, read the [Lightning in 15 minues](https://lightning.ai/docs/pytorch/stable/starter/introduction.html) section of the PyTorch Lightning framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a LightningModule with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from models.mlp import MultiLayerPerceptron\n",
    "\n",
    "# define the lightning module to train our model\n",
    "class LitModule(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = MultiLayerPerceptron(input_features = 360, \n",
    "                             hidden_size = 64,\n",
    "                             num_classes = 6)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Training_step defines the train loop.\n",
    "        # Just compute and return the loss. The trainer will take care of zeroing \n",
    "        # the gradient, computing the gradients and updating the weights.\n",
    "        X, y = batch\n",
    "        logits = self.model(X)\n",
    "        # Compute the loss\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(params=self.model.parameters(), lr=0.1)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create the model\n",
    "model = MultiLayerPerceptron(input_features = 360, \n",
    "                             hidden_size = 64,\n",
    "                             num_classes = 6)\n",
    "\n",
    "# Create the lightning module and initialize it with our model.\n",
    "lit_model = LitModule(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name    | Type                 | Params\n",
      "-------------------------------------------------\n",
      "0 | model   | MultiLayerPerceptron | 23.5 K\n",
      "1 | loss_fn | CrossEntropyLoss     | 0     \n",
      "-------------------------------------------------\n",
      "23.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 K    Total params\n",
      "0.094     Total estimated model params size (MB)\n",
      "/opt/homebrew/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1dfe5a5dc7414588426f20579291f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "#  - We will use the train data loader\n",
    "trainer = L.Trainer(max_epochs=10, accelerator=\"cpu\", log_every_n_steps=1)\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training Accuracy = 83.33 % (50/60)\n",
      "   Validation Accuracy = 33.33 % (8/24)\n"
     ]
    }
   ],
   "source": [
    "model = lit_model.model\n",
    "\n",
    "model.eval()\n",
    "report_accuracy(model, train_dl, prefix=\"   Training \")\n",
    "report_accuracy(model, val_dl, prefix=\"   Validation \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
